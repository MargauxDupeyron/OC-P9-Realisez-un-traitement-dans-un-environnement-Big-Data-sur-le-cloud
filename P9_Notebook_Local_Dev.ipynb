{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca146e79-1b8c-4448-b14e-eed8d632f665",
   "metadata": {},
   "source": [
    "# P9 – Déploiement d’un pipeline de featurisation d’images sur AWS EMR (PySpark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441159cc",
   "metadata": {},
   "source": [
    "**Sommaire** :\n",
    "\n",
    "**1. Préambule**<br />\n",
    "&emsp;1.1 Problématique<br />\n",
    "&emsp;1.2 Objectifs dans ce projet<br />\n",
    "&emsp;1.3 Déroulement des étapes du projet<br />\n",
    "**2. Choix techniques généraux retenus**<br />\n",
    "&emsp;2.1 Calcul distribué<br />\n",
    "&emsp;2.2 Transfert Learning<br />\n",
    "**3. Déploiement de la solution en local**<br />\n",
    "&emsp;3.1 Environnement de travail<br />\n",
    "&emsp;3.2 Installation de Spark<br />\n",
    "&emsp;3.3 Installation des packages<br />\n",
    "&emsp;3.4 Import des librairies<br />\n",
    "&emsp;3.5 Définition des PATH pour charger les images et enregistrer les résultats<br />\n",
    "&emsp;3.6 Création de la SparkSession<br />\n",
    "&emsp;3.7 Traitement des données<br />\n",
    "&emsp;&emsp;3.7.1 Chargement des données<br />\n",
    "&emsp;&emsp;3.7.2 Préparation du modèle<br />\n",
    "&emsp;&emsp;3.7.3 Définition du processus de chargement des images et application <br />\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;de leur featurisation à travers l'utilisation de pandas UDF<br />\n",
    "&emsp;&emsp;3.7.4 Exécution des actions d'extractions de features<br />\n",
    "&emsp;3.8 Chargement des données enregistrées et validation du résultat<br />\n",
    "&emsp;3.9 Réduction de dimension par PCA en PySpark<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e72130",
   "metadata": {},
   "source": [
    "# 1. Préambule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3cd0da",
   "metadata": {},
   "source": [
    "## 1.1. Problématique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd822e32",
   "metadata": {},
   "source": [
    "La très jeune start-up de l'AgriTech, nommée \"**Fruits**!\", cherche à proposer des solutions innovantes pour la récolte des fruits. La volonté de l’entreprise est de préserver la biodiversité des fruits en permettant des traitements spécifiques pour chaque espèce de fruits en développant des robots cueilleurs intelligents. La start-up souhaite dans un premier temps se faire connaître en mettant à disposition du grand public une application mobile qui permettrait aux utilisateurs de prendre en photo un fruit et d'obtenir des informations sur ce fruit. Pour la start-up, cette application permettrait de sensibiliser le grand public à la biodiversité des fruits et de mettre en place une première version du moteur de classification des images de fruits.\n",
    "De plus, le développement de l’application mobile permettra de construire une première version de l'architecture **Big Data** nécessaire."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0407121a",
   "metadata": {},
   "source": [
    "## 1.2. Objectifs dans ce projet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2cee08",
   "metadata": {},
   "source": [
    "1. Développer une première chaîne de traitement des données qui comprendra le **preprocessing** et une étape de **réduction de dimension**.\n",
    "2. Tenir compte du fait que <u>le volume de données va augmenter très rapidement</u> après la livraison de ce projet, ce qui implique de:\n",
    " - Déployer le traitement des données dans un environnement **Big Data**\n",
    " - Développer les scripts en **pyspark** pour effectuer du **calcul distribué**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f999b236",
   "metadata": {},
   "source": [
    "## 1.3. Déroulement des étapes du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b95e6ce",
   "metadata": {},
   "source": [
    "Le projet va être réalisé en 2 temps, dans deux environnements différents. Nous allons dans un premier temps développer et exécuter notre code en local, en travaillant sur un nombre limité d'images à traiter.\n",
    "\n",
    "Une fois les choix techniques validés, nous déploierons notre solution dans un environnement Big Data en mode distribué.\n",
    "\n",
    "<u>Pour cette raison, ce projet sera divisé en 3 parties</u>:\n",
    "1. Liste des choix techniques généraux retenus\n",
    "2. Déploiement de la solution en local\n",
    "3. Déploiement de la solution dans le cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b34029",
   "metadata": {},
   "source": [
    "# 2. Choix techniques généraux retenus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568d22e",
   "metadata": {},
   "source": [
    "## 2.1 Calcul distribué"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32baf092",
   "metadata": {},
   "source": [
    "L’énoncé du projet nous impose de développer des scripts en **pyspark** afin de <u>prendre en compte l’augmentation très rapide du volume de donné après la livraison du projet</u>.\n",
    "\n",
    "Pour comprendre rapidement et simplement ce qu’est **pyspark** et son principe de fonctionnement, nous vous conseillons de lire cet article : [PySpark : Tout savoir sur la librairie Python](https://datascientest.com/pyspark)\n",
    "\n",
    "<u>Le début de l’article nous dit ceci </u>:« *Lorsque l’on parle de traitement de bases de données sur python, on pense immédiatement à la librairie pandas. Cependant, lorsqu’on a affaire à des bases de données trop massives, les calculs deviennent trop lents. Heureusement, il existe une autre librairie python, assez proche de pandas, qui permet de traiter des très grandes quantités de données : PySpark.Apache Spark est un framework open-source développé par l’AMPLab de UC Berkeley permettant de traiter des bases de données massives en utilisant le calcul distribué, technique qui consiste à exploiter plusieurs unités de calcul réparties en clusters au profit d’un seul projet afin de diviser le temps d’exécution d’une requête.Spark a été développé en Scala et est au meilleur de ses capacités dans son langage natif. Cependant, la librairie PySpark propose de l’utiliser avec le langage Python, en gardant des performances similaires à des implémentations en Scala.Pyspark est donc une bonne alternative à la librairie pandas lorsqu’on cherche à traiter des jeux de données trop volumineux qui entraînent des calculs trop chronophages.* »\n",
    "\n",
    "Comme nous le constatons, **pySpark** est un moyen de communiquer avec **Spark** via le langage **Python**.**Spark**, quant à lui, est un outil qui permet de gérer et de coordonner l'exécution de tâches sur des données à travers un groupe d'ordinateurs. <u>Spark (ou Apache Spark) est un framework open source de calcul distribué in-memory pour le traitement et l'analyse de données massives</u>.\n",
    "\n",
    "Un autre [article très intéressant et beaucoup plus complet pour comprendre le **fonctionnement de Spark**](https://www.veonum.com/apache-spark-pour-les-nuls/), ainsi que le rôle des **Spark Session** que nous utiliserons dans ce projet.\n",
    "\n",
    "<u>Voici également un extrait</u>:\n",
    "\n",
    "*Les applications Spark se composent d’un pilote (« driver process ») et de plusieurs exécuteurs (« executor processes »). Il peut être configuré pour être lui-même l’exécuteur (local mode) ou en utiliser autant que nécessaire pour traiter l’application, Spark prenant en charge la mise à l’échelle automatique par une configuration d’un nombre minimum et maximum d’exécuteurs.*\n",
    "\n",
    "![Schéma de Spark](img/spark-schema.png)\n",
    "\n",
    "*Le driver (parfois appelé « Spark Session ») distribue et planifie les tâches entre les différents exécuteurs qui les exécutent et permettent un traitement réparti. Il est le responsable de l’exécution du code sur les différentes machines.\n",
    "\n",
    "Chaque exécuteur est un processus Java Virtual Machine (JVM) distinct dont il est possible de configurer le nombre de CPU et la quantité de mémoire qui lui est alloué. Une seule tâche peut traiter un fractionnement de données à la fois.*\n",
    "\n",
    "Dans les deux environnements (Local et Cloud) nous utiliserons donc **Spark** et nous l’exploiterons à travers des scripts python grâce à **PySpark**.\n",
    "\n",
    "Dans la <u>version locale</u> de notre script nous **simulerons le calcul distribué** afin de valider que notre solution fonctionne.Dans la <u>version cloud</u> nous **réaliserons les opérations sur un cluster de machine**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a48cf",
   "metadata": {},
   "source": [
    "## 2.2 Transfert Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5364c9f9",
   "metadata": {},
   "source": [
    "L'énoncé du projet nous demande également de réaliser une première chaîne de traitement des données qui comprendra le preprocessing et une étape de réduction de dimension. Il est également précisé qu'il n'est pas nécessaire d'entraîner un modèle pour le moment.\n",
    "\n",
    "Nous décidons de partir sur une solution de **transfert learning**. Simplement, le **transfert learning** consiste à utiliser la connaissance déjà acquise par un modèle entraîné (ici **MobileNetV2**) pour l'adapter à notre problématique. Nous allons fournir au modèle nos images, et nous allons <u>récupérer l'avant dernière couche</u> du modèle.En effet la dernière couche de modèle est une couche softmax qui permet la classification des images ce que nous ne souhaitons pas dans ce projet. L'avant dernière couche correspond à un **vecteur réduit** de dimension (1,1,1280). Cela permettra de réaliser une première version du moteur pour la classification des images des fruits.\n",
    "\n",
    "**MobileNetV2** a été retenu pour sa <u>rapidité d'exécution</u>, particulièrement adaptée pour le traitement d'un gros volume de données ainsi que la <u>faible dimensionnalité du vecteur de caractéristique en sortie</u> (1,1,1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52d55a4",
   "metadata": {},
   "source": [
    "# 3. Déploiement de la solution en local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ccc1ee",
   "metadata": {},
   "source": [
    "## 3.1 Environnement de travail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1ae38f",
   "metadata": {},
   "source": [
    "Pour des raisons de simplicité, nous développons dans un environnement Linux Unbuntu (exécuté depuis une machine Windows dans une machine virtuelle)\n",
    "* Pour installer une machine virtuelle :  https://www.malekal.com/meilleurs-logiciels-de-machine-virtuelle-gratuits-ou-payants/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d7659e",
   "metadata": {},
   "source": [
    "## 3.2 Installation de Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c33f4",
   "metadata": {},
   "source": [
    "[La première étape consiste à installer Spark ](https://computingforgeeks.com/how-to-install-apache-spark-on-ubuntu-debian/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a1fa13",
   "metadata": {},
   "source": [
    "## 3.3 Installation des packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a7ac2a",
   "metadata": {},
   "source": [
    "<u>On installe ensuite à l'aide de la commande **pip** <br />\n",
    "les packages qui nous seront nécessaires</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d9256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install Pandas pillow tensorflow pyspark pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a43845",
   "metadata": {},
   "source": [
    "## 3.4 Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0c74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split, element_at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ff67c",
   "metadata": {},
   "source": [
    "## 3.5 Définition des PATH pour charger les images et enregistrer les résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b3791e",
   "metadata": {},
   "source": [
    "Dans cette version locale nous partons du principe que les données sont stockées dans le même répertoire que le notebook. Nous n'utilisons qu'un extrait de **300 images** à traiter dans cette première version en local.L'extrait des images à charger est stockée dans le dossier **Test1**. Nous enregistrerons le résultat de notre traitement dans le dossier \"**Results_Local**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0aa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = os.getcwd()\n",
    "\n",
    "PATH_DATA = os.path.join(BASE_PATH, \"data\", \"Test1\")\n",
    "PATH_RESULT = os.path.join(BASE_PATH, \"data\", \"Results_local\")\n",
    "\n",
    "os.makedirs(PATH_RESULT, exist_ok=True)\n",
    "\n",
    "print(f\"PATH_DATA   : {PATH_DATA}\")\n",
    "print(f\"PATH_RESULT : {PATH_RESULT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e637a",
   "metadata": {},
   "source": [
    "## 3.6 Création de la SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74736008",
   "metadata": {},
   "source": [
    "L’application Spark est contrôlée grâce à un processus de pilotage (driver process) appelé **SparkSession**. Une instance de **SparkSession** est la façon dont Spark exécute les fonctions définies par l’utilisateur dans l’ensemble du cluster. Une SparkSession correspond toujours à une application Spark.\n",
    "\n",
    "Ici nous créons une session spark en spécifiant dans l'ordre :\n",
    "  1. un **nom pour l'application**, qui sera affichée dans l'interface utilisateur Web Spark \"**P8**\"\n",
    "  2. que l'application doit s'exécuter **localement**. Nous ne définissons pas le nombre de cœurs à utiliser (comme .master('local[4]) pour 4 cœurs à utiliser), nous utiliserons donc tous les cœurs disponibles dans notre processeur.\n",
    "  3. une option de configuration supplémentaire permettant d'utiliser le **format \"parquet\"** que nous utiliserons pour enregistrer et charger le résultat de notre travail.\n",
    "  4. vouloir **obtenir une session spark** existante ou si aucune n'existe, en créer une nouvelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d7bdff",
   "metadata": {},
   "source": [
    "Nous exécutons Spark en mode local avec l’ensemble des cœurs CPU afin de simuler un calcul distribué tout en restant dans un environnement de développement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bea157",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder                                        \n",
    "    .appName(\"P9_Fruits_Image_Featurization\")                   # définition du nom de l'application spark (apparait dans le spark UI, les logs, cluster manager si cloud)\n",
    "                                                                # Le nom de l’application permet d’identifier clairement le job Spark dans l’interface de monitoring\n",
    "    .master(\"local[*]\")                                         # local = execution sur une seule machine ; [*] = utilise tous les coeurs CPU dispo\n",
    "    .config(\"spark.sql.parquet.writeLegacyFormat\", \"true\")      # utilisation d'un format Parquet compatible avec d'anciennes implémentations\n",
    "    .getOrCreate()                                              # si il existe une session il l'utilise sinon il en crée une nouvelle\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be303741",
   "metadata": {},
   "source": [
    "- 127.0.1.1 = adresse loopback (la machine qui parle à elle-même)\n",
    "- Spark préfère une vraie adresse réseau, même en local\n",
    "- 10.0.2.15 = IP interne de ta VM (VirtualBox / réseau NAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eca4208",
   "metadata": {},
   "source": [
    "Il y a deux niveaux dans Spark :\n",
    "- Niveau moderne (celui qu'on utilise)\n",
    "    - SparkSession\n",
    "    - DataFrames\n",
    "    - Spark SQL\n",
    "- Niveau bas niveau (historique)\n",
    "    - SparkContext\n",
    "    - RDD\n",
    "    - gestion directe des partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8b53ac",
   "metadata": {},
   "source": [
    "<u>Nous créons également la variable \"**sc**\" qui est un **SparkContext** issue de la variable **spark**</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aeccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# À partir de la SparkSession que j’ai déjà créée, je récupère le moteur interne Spark.\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a086010",
   "metadata": {},
   "source": [
    "<u>Affichage des informations de Spark en cours d'execution</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97bf13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fd01af",
   "metadata": {},
   "source": [
    "La SparkSession est correctement initialisée en mode local, avec un SparkContext actif. L’interface Spark UI est accessible, ce qui confirme que l’application Spark est bien lancée et prête à exécuter des transformations distribuées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a88b0",
   "metadata": {},
   "source": [
    "## 3.7 Traitement des données\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bc6619",
   "metadata": {},
   "source": [
    "Dans la suite de notre flux de travail, nous allons successivement:\n",
    "1. Préparer nos données\n",
    "    1. Importer les images dans un dataframe **pandas UDF**\n",
    "    2. Associer aux images leur **label**\n",
    "    3. Préprocesser en **redimensionnant nos images pour  qu'elles soient compatibles avec notre modèle**\n",
    "2. Préparer notre modèle\n",
    "    1. Importer le modèle **MobileNetV2**\n",
    "    2. Créer un **nouveau modèle** dépourvu de la dernière couche de MobileNetV2\n",
    "3. Définir le processus de chargement des images et l'application de leur featurisation à travers l'utilisation de pandas UDF\n",
    "3. Exécuter les actions d'extraction de features\n",
    "4. Enregistrer le résultat de nos actions\n",
    "5. Tester le bon fonctionnement en chargeant les données enregistrées"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853bd6ca",
   "metadata": {},
   "source": [
    "### 3.7.0 Création du jeu de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf754a4",
   "metadata": {},
   "source": [
    "Pour ce projet, nous utilisons la version Fruits-360 en 100×100 pixels, qui est stable, bien documentée et adaptée à un pipeline de traitement distribué.\n",
    "La version “original size” étant explicitement indiquée comme non finalisée par les auteurs, nous ne l’utilisons pas afin de rester dans un cadre maîtrisé et conforme aux objectifs du projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a5c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dossier source : version 100x100 stable du dataset\n",
    "SOURCE_DIR = os.path.join(\n",
    "    BASE_PATH,\n",
    "    \"data/fruits-360_dataset/fruits-360/Training\"\n",
    ")\n",
    "\n",
    "TARGET_DIR = PATH_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CLASSES = 30          # nombre de classes à garder\n",
    "NB_IMAGES_PER_CLASS = 10 # images par classe (~300 images au total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d289a4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(TARGET_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81effec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération de toutes les classes disponibles\n",
    "all_classes = sorted([\n",
    "    d for d in os.listdir(SOURCE_DIR)\n",
    "    if os.path.isdir(os.path.join(SOURCE_DIR, d))\n",
    "])\n",
    "\n",
    "# Sélection des classes\n",
    "selected_classes = random.sample(all_classes, NB_CLASSES)\n",
    "\n",
    "print(f\"Classes sélectionnées ({len(selected_classes)}) : {selected_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b06b40",
   "metadata": {},
   "source": [
    "Copie des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a730cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_copied = 0\n",
    "\n",
    "for cls in selected_classes:\n",
    "    src_dir = os.path.join(SOURCE_DIR, cls)\n",
    "    tgt_dir = os.path.join(TARGET_DIR, cls)\n",
    "\n",
    "    os.makedirs(tgt_dir, exist_ok=True)\n",
    "\n",
    "    # Liste complète des images de la classe\n",
    "    images = [\n",
    "        f for f in os.listdir(src_dir)\n",
    "        if f.lower().endswith(\".jpg\")\n",
    "    ]\n",
    "\n",
    "    # Sélection aléatoire des images de la classe\n",
    "    selected_images = random.sample(images, NB_IMAGES_PER_CLASS)\n",
    "\n",
    "    for img in selected_images:\n",
    "        shutil.copy(\n",
    "            os.path.join(src_dir, img),\n",
    "            os.path.join(tgt_dir, img)\n",
    "        )\n",
    "\n",
    "    total_copied += len(selected_images)\n",
    "    print(f\"{cls} : {len(selected_images)} images copiées\")\n",
    "\n",
    "print(f\"\\nTotal images copiées : {total_copied}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386fe0bc",
   "metadata": {},
   "source": [
    "### 3.7.1 Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a53244d",
   "metadata": {},
   "source": [
    "Les images sont chargées au format binaire, ce qui offre, plus de souplesse dans la façon de prétraiter les images. Avant de charger les images, nous spécifions que nous voulons charger uniquement les fichiers dont l'extension est **jpg**. Nous indiquons également de charger tous les objets possibles contenus dans les sous-dossiers du dossier communiqué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68e53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = (\n",
    "    spark.read.format(\"binaryFile\")         # spark lit des fichiers binaires\n",
    "    .option(\"pathGlobFilter\", \"*.jpg\")      # ne charge que les fichiers .jpg\n",
    "    .option(\"recursiveFileLookup\", \"true\")  # parcourt tous les sous-dossiers récursivement (ne lit pas que le dossier racine)\n",
    "    .load(PATH_DATA)                        # Spark scanne le dossier, crée un df distribué avec une ligne par image\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645faeaf",
   "metadata": {},
   "source": [
    "Chaque ligne contient : \n",
    "| Colonne            | Contenu                    |\n",
    "| ------------------ | -------------------------- |\n",
    "| `path`             | chemin complet du fichier  |\n",
    "| `modificationTime` | date de dernière modif     |\n",
    "| `length`           | taille en octets           |\n",
    "| `content`          | image en binaire (`bytes`) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863981e5",
   "metadata": {},
   "source": [
    "<u>Je ne conserve que le **path** de l'image et j'ajoute une colonne contenant les **labels** de chaque image</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b0494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction du label depuis le chemin\n",
    "images = images.withColumn(\n",
    "    \"label\",\n",
    "    element_at(split(col(\"path\"), \"/\"), -2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedae587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schéma du DataFrame\n",
    "images.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c3aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aperçu des données\n",
    "images.select(\"path\", \"label\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d47705",
   "metadata": {},
   "source": [
    "### 3.7.2 Préparation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26231b1b",
   "metadata": {},
   "source": [
    "Je vais utiliser la technique du **transfert learning** pour extraire les features des images. J'ai choisi d'utiliser le modèle **MobileNetV2** pour sa rapidité d'exécution comparée à d'autres modèles comme *VGG16* par exemple.\n",
    "\n",
    "Pour en savoir plus sur la conception et le fonctionnement de MobileNetV2, je vous invite à lire [cet article](https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c).\n",
    "\n",
    "<u>Voici le schéma de son architecture globale</u> : \n",
    "\n",
    "![Architecture de MobileNetV2](img/mobilenetv2_architecture.png)\n",
    "\n",
    "Il existe une dernière couche qui sert à classer les images selon 1000 catégories que nous ne voulons pas utiliser. L'idée dans ce projet est de récupérer le **vecteur de caractéristiques de dimensions (1,1,1280)** qui servira, plus tard, au travers d'un moteur de classification à reconnaitre les différents fruits du jeu de données.\n",
    "\n",
    "Comme d'autres modèles similaires, **MobileNetV2**, lorsqu'on l'utilise en incluant toutes ses couches, attend obligatoirement des images de dimension (224,224,3). Nos images étant toutes de dimension (100,100,3), nous devrons simplement les **redimensionner** avant de les confier au modèle.\n",
    "\n",
    "<u>Dans l'odre</u> :\n",
    " 1. Nous chargeons le modèle **MobileNetV2** avec les poids **précalculés** issus d'**imagenet** et en spécifiant le format de nos images en entrée\n",
    " 2. Nous créons un nouveau modèle avec:\n",
    "  - <u>en entrée</u> : l'entrée du modèle MobileNetV2\n",
    "  - <u>en sortie</u> : l'avant dernière couche du modèle MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdd9bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du modèle pré-entraîné SANS la tête de classification\n",
    "base_model = MobileNetV2(           # Construction de l'architecture MobileNetV2\n",
    "    weights=\"imagenet\",             # Initialisation du modèle avec des poids déjà appris sur ImageNet\n",
    "    include_top=False,              # Exclusion de la dernière partie du modèle qui correspond à la classification qu'on ne veut pas \n",
    "    input_shape=(224, 224, 3),      # Toutes les images envoyées au modèle auront cette forme (contrainte technique du modèle)\n",
    "    pooling=\"avg\"                   # Sortie (1280,): on supprime les dimensions spatiales, chaque image devient un vecteur de 1280 nombres\n",
    ")\n",
    "\n",
    "base_model.trainable = False        # On gèle les poids du modèle (pas d'entrainement, pas de mise a jour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd64f045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification de la sortie \n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f09a7d9",
   "metadata": {},
   "source": [
    "**Résumé** : \n",
    "\n",
    "- Entrée du modèle: \n",
    "    - `input_layer (InputLayer)` | `Output Shape: (None, 224, 224, 3)`\n",
    "    - Le modèle attend bien des images 224 x 224 avec 3 canaux de couleur / None = taille du batch inconnue à l'avance.\n",
    "\n",
    "- Corps du modèle : \n",
    "    - Longue liste de couches comme `Conv2D`, `DepthwiseConv2D`, `BatchNormalization`, `ReLU`... \n",
    "    - Ce bloc correspond à l'extraction progressive de caractéristiques visuelles. \n",
    "    - Il y a beaucoup de couches parce que MobileNetV2 utilise des bloc inversés résiduels, des convolutions depthwise, des skip connections -> ce qui sert à réduire le nombre de paramètres, garder de bonnes performances et être rapide. \n",
    "\n",
    "- Sortie : \n",
    "    - `global_average_pooling (GlobalAveragePooling2D)` | `Output Shape: (None, 1280)`\n",
    "    - Chaque image devient un vecteur de 1280 valeurs, sans information spatiale. \n",
    "\n",
    "**Les paramètres** :\n",
    "\n",
    "- ~2.2M de paramètres dont aucun entrainement (prouve bien que le transfert learning est figé)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0adcf5",
   "metadata": {},
   "source": [
    "Tous les workeurs doivent pouvoir accéder au modèle ainsi qu'à ses poids. Une bonne pratique consiste à charger le modèle sur le driver puis à diffuser ensuite les poids aux différents workeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc53ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère tous les paramètres numériques du modèle sous forme de listes de tableaux numpy\n",
    "# Ces données doivent être envoyées à tous les workers de manière efficace\n",
    "broadcast_weights = sc.broadcast(base_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc0e34e",
   "metadata": {},
   "source": [
    "<u>Mettons cela sous forme de fonction</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd51ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Creates a MobileNetV2 model (feature extractor)\n",
    "    with broadcasted ImageNet weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(\n",
    "        weights=None,\n",
    "        include_top=False,\n",
    "        input_shape=(224, 224, 3),\n",
    "        pooling=\"avg\"\n",
    "    )\n",
    "    \n",
    "    model.set_weights(broadcast_weights.value)  # Injection des poids broadcastés -> chaque worker a son propre modèle avec les bons poids\n",
    "    model.trainable = False     # Le modèle ne sera jamais entrainé \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b62e4",
   "metadata": {},
   "source": [
    "Le modèle est chargé une seule fois sur le driver, puis ses poids sont diffusés aux workers Spark à l’aide d’un broadcast. Chaque worker reconstruit localement le modèle avec ces poids, ce qui évite des rechargements coûteux et permet une exécution distribuée efficace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5620876",
   "metadata": {},
   "source": [
    "### 3.7.3 Définition du processus de chargement des images et application de leur featurisation à travers l'utilisation de pandas UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2a1fe4",
   "metadata": {},
   "source": [
    "Le but de cette section est de prendre des images stockées sous forme binaire dans un DataFrame Spark et les transformer en vecteurs de caractéristiques,en parallèle, sur plusieurs workers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3ab469",
   "metadata": {},
   "source": [
    "MobileNetV2 est un modèle Python / TensorFlow et Spark est distribué. Il faut donc un pont entre Spark, Pandas et TensorFlow => Pandas UDF. \n",
    "\n",
    "<u>L'empilement des appels est la suivante</u> : \n",
    "Pandas UDF \n",
    "- featuriser une série d'images pd.Series \n",
    "- prétraiter une image\n",
    "\n",
    "Spark :\n",
    "- découpe les données\n",
    "- envoie des batches à chaque worker\n",
    "- chaque batch est traité via Pandas + TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe99df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nécessaires\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from typing import Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Préprocessing image\n",
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocess raw image bytes for MobileNetV2 inference.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).convert(\"RGB\")\n",
    "    img = img.resize((224, 224))\n",
    "    arr = img_to_array(img)     # convertit une image PIL en tableau numpy (224, 224, 3)\n",
    "    arr = preprocess_input(arr) # applique la normalisation spécifique MobileNetV2\n",
    "    return arr\n",
    "\n",
    "#Featurisation d'un batch Pandas\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pandas Series of image bytes using the provided model.\n",
    "    Returns a pandas Series of feature vectors.\n",
    "    \"\"\"\n",
    "    batch = np.stack(content_series.map(preprocess))\n",
    "    features = model.predict(batch, verbose=0)\n",
    "\n",
    "    return pd.Series(features.tolist())\n",
    "\n",
    "# Pandas UDF (Spark – Scalar Iterator)\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def featurize_udf(\n",
    "    content_series_iter: Iterator[pd.Series]\n",
    ") -> Iterator[pd.Series]:\n",
    "    \"\"\"\n",
    "    Scalar Iterator Pandas UDF for distributed image featurization.\n",
    "    \"\"\"\n",
    "    model = model_fn()  # chargé UNE fois par worker\n",
    "\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)   # pour chaque batch : on applique MobileNetV2, on renvoie les features et Spark récupère le résultat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdf2ef9",
   "metadata": {},
   "source": [
    "### 3.7.4 Exécution des actions d'extraction de features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e0c59",
   "metadata": {},
   "source": [
    "Les Pandas UDF, sur de grands enregistrements (par exemple, de très grandes images), peuvent rencontrer des erreurs de type Out Of Memory (OOM). Si vous rencontrez de telles erreurs dans la cellule ci-dessous, essayez de réduire la taille du lot Arrow via 'maxRecordsPerBatch'\n",
    "\n",
    "Je n'utiliserai pas cette commande dans ce projet et je laisse donc la commande en commentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f30d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# À activer uniquement en cas d'erreurs OOM sur de très gros volumes\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8f95d",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant exécuter la featurisation sur l'ensemble de notre DataFrame Spark.\n",
    "\n",
    "<u>REMARQUE</u> : Cela peut prendre beaucoup de temps, tout dépend du volume de données à traiter. \n",
    "\n",
    "Notre jeu de données de **Test** contient **22819 images**. \n",
    "Cependant, dans l'exécution en mode **local**, \n",
    "nous <u>traiterons un ensemble réduit de **300 images**</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c1767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = (\n",
    "    images              # DataFrame Spark initial (contient path, content, label)\n",
    "    .repartition(20)    # nombre de partitions explicite (local / cluster)\n",
    "    .select(\n",
    "        col(\"path\"),    # on conserve le chemin de l'image\n",
    "        col(\"label\"),   # on conserve le label de l'image\n",
    "        featurize_udf(col(\"content\")).alias(\"features\")     # pour chaque image, on applique la Pandas UDF sur la colonne content\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1bf5c5",
   "metadata": {},
   "source": [
    "Concrètement :\n",
    "\n",
    "- Spark :\n",
    "    - regroupe les lignes par batch\n",
    "    - envoie les batches aux workers\n",
    "- Chaque worker :\n",
    "    - charge MobileNetV2 une seule fois\n",
    "    - applique le preprocessing\n",
    "    - extrait les features\n",
    "- Le résultat :\n",
    "    - une colonne features\n",
    "    - contenant un vecteur de 1280 floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification rapide du schéma\n",
    "features_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5e83ec",
   "metadata": {},
   "source": [
    "Rappel du PATH où seront inscrits les fichiers au format \"**parquet**\" contenant nos résultats, à savoir, un DataFrame contenant 3 colonnes:\n",
    " 1. Path des images\n",
    " 2. Label de l'image\n",
    " 3. Vecteur de caractéristiques de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fcdb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Résultats enregistrés dans : {PATH_RESULT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8901db3",
   "metadata": {},
   "source": [
    "Execution : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d07466",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    features_df\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(PATH_RESULT)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9506f21",
   "metadata": {},
   "source": [
    "## 3.8 Chargement des données enregistrées et validation du résultat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d66fcf",
   "metadata": {},
   "source": [
    "\n",
    "<u>On charge les données</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19243bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = spark.read.parquet(PATH_RESULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dbcfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification du schéma \n",
    "features_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca49f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification dimension du vecteur\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "features_df.select(size(\"features\").alias(\"features_dim\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f15070",
   "metadata": {},
   "source": [
    "<u>On affiche les 5 premières lignes du DataFrame</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1bcdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df.select(\"label\", \"features\").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5348d",
   "metadata": {},
   "source": [
    "Nous venons de valider le processus sur un jeu de données allégé en local où nous avons simulé un cluster de machines en répartissant la charge de travail sur différents cœurs de processeur au sein d'une même machine.\n",
    "\n",
    "Nous allons maintenant généraliser le processus en déployant notre solution sur un réel cluster de machines et nous travaillerons désormais sur la totalité des 22819 images de notre dossier \"Test\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa7ebca",
   "metadata": {},
   "source": [
    "## 3.9 Réduction de dimension par PCA en PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f107d56b",
   "metadata": {},
   "source": [
    "### 3.9.1 Vectorisation des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a3a156",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import array_to_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2538f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_vec_df = features_df.withColumn(\n",
    "    \"features_vec\",\n",
    "    array_to_vector(\"features\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94255f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_vec_df.select(\"features_vec\").show(1, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fc4b85",
   "metadata": {},
   "source": [
    "### 3.9.2 Application de la PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e17251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45d4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full = PCA(\n",
    "    k=1280,   # dimension originale des embeddings MobileNetV2\n",
    "    inputCol=\"features_vec\",\n",
    "    outputCol=\"features_pca_full\"\n",
    ")\n",
    "\n",
    "pca_full_model = pca_full.fit(features_vec_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa5762",
   "metadata": {},
   "source": [
    "### 3.9.3 Analyse de la variance expliquée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca_full_model.explainedVariance.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5fe829",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var = pd.DataFrame({\n",
    "    \"component\": np.arange(1, len(explained_variance) + 1),\n",
    "    \"explained_variance\": explained_variance,\n",
    "})\n",
    "\n",
    "df_var[\"cum_variance\"] = df_var[\"explained_variance\"].cumsum()\n",
    "\n",
    "df_var.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b172421",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_80 = (df_var[\"cum_variance\"] >= 0.80).idxmax() + 1\n",
    "k_90 = (df_var[\"cum_variance\"] >= 0.90).idxmax() + 1\n",
    "\n",
    "print(\"k pour 80% de variance :\", k_80)\n",
    "print(\"k pour 90% de variance :\", k_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf982e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# On limite l'affichage aux 100 premières composantes pour la lisibilité\n",
    "df_plot = df_var.iloc[:100]\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Barres : variance expliquée par composante\n",
    "plt.bar(df_plot[\"component\"], df_plot[\"explained_variance\"], alpha=0.7, label=\"Variance expliquée\")\n",
    "\n",
    "# Courbe : variance cumulée\n",
    "plt.plot(df_plot[\"component\"], df_plot[\"cum_variance\"], \n",
    "         color=\"red\", marker=\"o\", linewidth=2, label=\"Variance cumulée\")\n",
    "\n",
    "# Lignes horizontales de seuil\n",
    "plt.axhline(y=0.8, color=\"green\", linestyle=\"--\", label=\"Seuil 80%\")\n",
    "plt.axhline(y=0.9, color=\"purple\", linestyle=\"--\", label=\"Seuil 90%\")\n",
    "\n",
    "plt.xlabel(\"Composantes principales\")\n",
    "plt.ylabel(\"Variance expliquée\")\n",
    "plt.title(\"Scree plot – PCA sur les embeddings MobileNetV2\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2e0f78",
   "metadata": {},
   "source": [
    "L’analyse de la variance expliquée par la PCA montre que les premières composantes principales concentrent l’essentiel de l’information contenue dans les embeddings extraits par MobileNetV2.\n",
    "En particulier, environ 80 % de la variance totale est expliquée par les ~50 premières composantes, et 90 % par les ~100 premières composantes.\n",
    "Afin de réduire significativement la dimension tout en conservant l’essentiel de l’information discriminante, nous retenons un compromis de k = 50 composantes pour la suite du pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccabcc26",
   "metadata": {},
   "source": [
    "### 3.9.4 Application de la PCA finale avec un k justifié"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8104d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(\n",
    "    k=50,     \n",
    "    inputCol=\"features_vec\",\n",
    "    outputCol=\"features_pca\"\n",
    ")\n",
    "\n",
    "pca_model = pca.fit(features_vec_df)\n",
    "features_pca_df = pca_model.transform(features_vec_df)\n",
    "\n",
    "features_pca_df.select(\"features_pca\").show(3, truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "432.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
